# -*- coding: utf-8 -*-
"""ME19B084_Ashish_prediction_code

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13aAU2x0-o4zNzqAZQgWGyNxQmG57ELAI

# Importing the libraries and the dataset
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

import seaborn as sns
sns.set()
plt.style.use('fivethirtyeight')
# %matplotlib inline

df_test = pd.read_csv('iitm_test_data.csv')
df_train = pd.read_csv('iitm_train_data.csv')

"""# Dataset exploring Raw Data"""

df_train

df_train.shape

df_test

df_test.shape

"""# Checking missing data"""

df_train.isnull().sum()

df_test.isnull().sum()

df_train.columns

df_test.columns

df_train.iloc[:,:-1].describe()

"""# Data Distribution among classes"""

fig, ax=plt.subplots(1,2,figsize=(15,6))
_ = sns.countplot(x='class', data=df_train, ax=ax[0])
_ = df_train['class'].value_counts().plot.pie(autopct="%1.1f%%", ax=ax[1])

df_train.iloc[:,:-1].hist(figsize=(20,20))
plt.show()

"""# Correlations Matrix"""

corr = df_train.iloc[:,:-1].corr()
fig = plt.figure(figsize=(20,20))
ax = fig.add_subplot(111)
ax = sns.heatmap(corr, annot=True, cmap = "coolwarm", fmt=".2f")

"""1.   Eventhough, there was no need of removing any operation settings or column because there was a very less overall impact on the prediction accuracy.
2.   Highly Co-related Columns are -
          1.   6,7 out of 6,7,8 column
          2.   30,31 out of 30,31,32 
          3.   33,34 out of 33,34,35
          4.   42,43 out of 42,43,44
          5.   45,46 out of 45,46,47

# Dropping Highly Corelated Column from traing dataset
"""

columns_names_train = list(df_train.columns)
not_required_feats = ["6", "7", "30", "31", "33", "34", "42", "43", "45", "46"]
feats_train = [feat for feat in columns_names_train if feat not in not_required_feats]
print(feats_train)

# df_train = df_train[feats_train]
df_train

"""# Dropping Highly Corelated Column from test dataset"""

columns_names_test = list(df_test.columns)
not_required_feats = ["6", "7", "30", "31", "33", "34", "42", "43", "45", "46"]
feats_test = [feat for feat in columns_names_test if feat not in not_required_feats]
print(feats_test)

# df_test = df_train[feats_test]
df_test

"""# Model Training

## Training Dataset
"""

X = df_train.iloc[:, :-1].values
Y = df_train.iloc[:, -1].values

from sklearn.model_selection import train_test_split
from sklearn import metrics
X_train,X_test,y_train,y_test = train_test_split(X, Y, stratify=Y, test_size = 0.1,random_state = 1)

def draw_confusion_matrix(cm):
    plt.figure(figsize=(12,8))
    sns.heatmap(cm,annot=True,fmt="d", center=0, cmap='autumn') 
    plt.title("Confusion Matrix")
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.show()

"""## LogisticRegression"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix

logreg = LogisticRegression(random_state=1)
logreg.fit(X_train, y_train)

y_predict_train_logreg = logreg.predict(X_train)
y_predict_test_logreg = logreg.predict(X_test)

train_accuracy_score_logreg = accuracy_score(y_train, y_predict_train_logreg)
test_accuracy_score_logreg = accuracy_score(y_test, y_predict_test_logreg)

print(train_accuracy_score_logreg)
print(test_accuracy_score_logreg)

cm_logreg = confusion_matrix(y_test,y_predict_test_logreg)
draw_confusion_matrix(cm_logreg)

"""## RandomForestClassifier"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix

rf = RandomForestClassifier(random_state=1, n_estimators=50, max_depth=6, criterion = 'entropy', 
                            min_samples_leaf= 1,min_samples_split= 2)
rf.fit(X_train, y_train)

y_predict_train_rf = rf.predict(X_train)
y_predict_test_rf = rf.predict(X_test)

train_accuracy_score_rf = accuracy_score(y_train, y_predict_train_rf)
test_accuracy_score_rf = accuracy_score(y_test, y_predict_test_rf)

print(train_accuracy_score_rf)
print(test_accuracy_score_rf)

cm_rf = confusion_matrix(y_test,y_predict_test_rf)
draw_confusion_matrix(cm_rf)

"""## XgBoost Algorithm"""

from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, confusion_matrix

xgb = XGBClassifier()
xgb.fit(X_train, y_train)

y_predict_train_xgb = xgb.predict(X_train)
y_predict_test_xgb = xgb.predict(X_test)

train_accuracy_score_xgb = accuracy_score(y_train, y_predict_train_xgb)
test_accuracy_score_xgb = accuracy_score(y_test, y_predict_test_xgb)

print(train_accuracy_score_xgb)
print(test_accuracy_score_xgb)

cm_xgb = confusion_matrix(y_test,y_predict_test_xgb)
draw_confusion_matrix(cm_xgb)

"""## Metrics calculation"""

Accuracy = metrics.accuracy_score(y_test, y_predict_test_xgb)
print(f"Accuracy:", Accuracy)

"""## Precision 

1. Macro averaged precision: calculate precision for all classes individually and then average them
2. Micro averaged precision: calculate class wise true positive and false positive and then use that to calculate overall precision

"""

macro_averaged_precision = metrics.precision_score(y_train, y_predict_train_xgb, average = 'macro')
print(f"Macro-Averaged Precision score for training data:", macro_averaged_precision)

macro_averaged_precision = metrics.precision_score(y_test, y_predict_test_xgb, average = 'macro')
print(f"Macro-Averaged Precision score:", macro_averaged_precision)

micro_averaged_precision = metrics.precision_score(y_test, y_predict_test_xgb, average = 'micro')
print(f"Micro-Averaged Precision score:", micro_averaged_precision)

"""## Recall

1. Macro averaged recall: calculate recall for all classes individually and then average them
2. Micro averaged recall: calculate class wise true positive and false negative and then use that to calculate overall recall

"""

macro_averaged_recall = metrics.recall_score(y_train, y_predict_train_xgb, average = 'macro')
print(f"Macro-averaged recall score for training data: {macro_averaged_recall}")

macro_averaged_recall = metrics.recall_score(y_test, y_predict_test_xgb, average = 'macro')
print(f"Macro-averaged recall score: {macro_averaged_recall}")

micro_averaged_recall = metrics.recall_score(y_test, y_predict_test_xgb, average = 'micro')
print(f"Micro-Averaged recall score: {micro_averaged_recall}")

"""## F1 Score"""

macro_averaged_f1 = metrics.f1_score(y_test, y_predict_test_xgb, average = 'macro')
print(f"Macro-Averaged F1 score: {macro_averaged_f1}")

micro_averaged_f1 = metrics.f1_score(y_test, y_predict_test_xgb, average = 'micro')
print(f"Micro-Averaged F1 score: {micro_averaged_f1}")

"""# Prediction of Test Dataset and saving into csv file"""

test_data = df_test.iloc[:,:].values
y_final = xgb.predict(test_data)

y_final
df_test["class"] = pd.DataFrame(y_final)
df_test.to_csv("ME19B084_Ashish_predictions.csv", index = False)